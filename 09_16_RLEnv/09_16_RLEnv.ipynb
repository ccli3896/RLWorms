{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time # delay within program\n",
    "from math import *\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "\n",
    "import nidaqmx # laser output\n",
    "from pyueye import ueye\n",
    "from pypyueye import Camera\n",
    "\n",
    "from improc import *\n",
    "from worm_env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking. \n",
    "This includes deterministic policy and simple averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random actions\n",
    "#### Program flow:\n",
    "* Initializes environment.\n",
    "* Collects background and sets target angle\n",
    "* Runs 4 episodes for 10 minutes (600 s) each:\n",
    "    * While episode is going:\n",
    "        * Randomly picks action\n",
    "        * Takes one step\n",
    "        * Adds results to a trajectory dictionary\n",
    "    * Prints number of steps taken during episode\n",
    "    * Saves entire trajectory to .json file as dictionary\n",
    "* Closes environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: when changing pixelclock at runtime, you may need to update the fps and exposure parameters\n",
      "Warning: Specified fps (20.00) not in possible range: [7.40, 14.92]. fps has been set to 14.92.\n",
      "Episode finished after 201 timesteps\n",
      "Episode finished after 201 timesteps\n",
      "No worm \t\t\r"
     ]
    }
   ],
   "source": [
    "# Save track, total reward history, observations, endpoints, images, action history\n",
    "fbase = 'Random_i0_'\n",
    "env = ProcessedWorm(0,ep_len=600)\n",
    "eps_per_worm = 4\n",
    "\n",
    "WORM_NUMBER = 0\n",
    "\n",
    "\n",
    "for i_episode in np.arange(eps_per_worm)+WORM_NUMBER*eps_per_worm:\n",
    "    done = False\n",
    "    fname = fbase+str(i_episode)+'.json'\n",
    "    trajectory = {}\n",
    "    \n",
    "    obs = env.reset(target=(i_episode*90)%360)\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        obs, r, done, info = env.step(action)\n",
    "        print(f'Body and head: {obs} \\t\\t\\r',end='')\n",
    "        \n",
    "        # Combining trajectory data in info with previous steps\n",
    "        add_to_traj(trajectory,info)\n",
    "        \n",
    "    print(\"Episode finished after {} timesteps\".format(info['t']))\n",
    "    \n",
    "    with open(fname,'wb') as f:\n",
    "        json.dump(trajectory,f)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic policy\n",
    "Same flow as random action above but picks actions according to known AIY policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_policy(obs):\n",
    "    # Returns 0 if obs is nans.\n",
    "    # Otherwise, returns the known deterministic policy.\n",
    "    if np.isnan(obs[0]):\n",
    "        return 0\n",
    "    body_dir,head_body = obs\n",
    "    if body_dir*head_body < 0:\n",
    "        action = 1\n",
    "    elif body_dir == 0:\n",
    "        if head_body == 0:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0\n",
    "    else:\n",
    "        action = 0\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbase = 'Known_i0_'\n",
    "env = ProcessedWorm(0,ep_len=600)\n",
    "eps_per_worm = 4\n",
    "\n",
    "WORM_NUMBER = 0\n",
    "\n",
    "\n",
    "for i_episode in np.arange(eps_per_worm)+WORM_NUMBER*eps_per_worm:\n",
    "    done = False\n",
    "    fname = fbase+str(i_episode)+'.json'\n",
    "    trajectory = {}\n",
    "    action = 0\n",
    "    \n",
    "    obs = env.reset(target=(i_episode*90)%360)\n",
    "    \n",
    "    while not done:\n",
    "        obs, r, done, info = env.step(action)\n",
    "        print(f'Body and head: {obs} \\t\\t\\r',end='')\n",
    "        \n",
    "        # Combining trajectory data in info with previous steps\n",
    "        add_to_traj(trajectory,info)\n",
    "        \n",
    "        # Chooses action for next step\n",
    "        action = det_policy(obs)\n",
    "        \n",
    "    print(\"Episode finished after {} timesteps\".format(info['t']))\n",
    "    \n",
    "    with open(fname,'wb') as f:\n",
    "        json.dump(trajectory,f)\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging rewards\n",
    "Initializes table randomly and takes averages as they're collected.  \n",
    "\n",
    "For every entry the update step is\n",
    "$$\n",
    "\\bar{r}_{i+1}\\leftarrow \\bar{r}_i +\\frac{1}{n}(r_n-\\bar{r}_i)\n",
    "$$\n",
    "\n",
    "Separate averages $\\bar{r}$ are maintained for every state-action pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### average in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbase = 'Ave_i0_'\n",
    "env = ProcessedWorm(0,ep_len=600)\n",
    "eps_per_worm = 4\n",
    "\n",
    "WORM_NUMBER = 0\n",
    "\n",
    "\n",
    "for i_episode in np.arange(eps_per_worm)+WORM_NUMBER*eps_per_worm:\n",
    "    done = False\n",
    "    fname = fbase+str(i_episode)+'.json'\n",
    "    trajectory = {}\n",
    "    action = 0\n",
    "    \n",
    "    obs = env.reset(target=(i_episode*90)%360)\n",
    "    \n",
    "    while not done:\n",
    "        obs, r, done, info = env.step(action)\n",
    "        print(f'Body and head: {obs} \\t\\t\\r',end='')\n",
    "        \n",
    "        # Combining trajectory data in info with previous steps\n",
    "        add_to_traj(trajectory,info)\n",
    "        \n",
    "        # Chooses action for next step\n",
    "        action = det_policy(obs)\n",
    "        \n",
    "    print(\"Episode finished after {} timesteps\".format(info['t']))\n",
    "    \n",
    "    with open(fname,'wb') as f:\n",
    "        json.dump(trajectory,f)\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r919",
   "language": "python",
   "name": "r919"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
