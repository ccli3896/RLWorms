{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting performances using reward differences only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import model_based_agent as mba \n",
    "import worm_env as we \n",
    "import ensemble_mod_env as eme\n",
    "\n",
    "from improc import *\n",
    "import utils as ut\n",
    "import tab_agents as tab\n",
    "from datetime import datetime \n",
    "\n",
    "def reward_diff_method(\n",
    "    collection_eps = 6,\n",
    "    frac_on = 1/2,\n",
    "    eval_on_list = [1/2,1/3,1/4],\n",
    "    collection_ep_time = 600, # in seconds. Must be a multiple of worm_ep_len\n",
    "    eval_ep_time = 120, # in seconds\n",
    "    worm_ep_len = 300, # in seconds\n",
    "    ):\n",
    "    '''\n",
    "    Function output:\n",
    "    Saves all trajectories for collection and eval episodes. \n",
    "    collect{i}.pkl for former and mod{i}_{eval frac ind}.pkl is the model after ep i.\n",
    "    eval{i}_{eval frac ind}.pkl for latter\n",
    "    \n",
    "    1. Collects data with light on frac_on of the time.\n",
    "    2. Evaluates reward difference policy with various amounts of light penalty given\n",
    "        by eval_on_list. \n",
    "    '''\n",
    "    \n",
    "    folder = './Data/Reals'+datetime.now().strftime('%d-%m-%H-%M')\n",
    "    fbase = folder+'/realworm_'\n",
    "    if os.path.isdir(folder):\n",
    "        os.path.rmdir(folder)\n",
    "    os.mkdir(folder)\n",
    "    \n",
    "    # Initialize objects\n",
    "    dh = mba.DataHandler()\n",
    "    worm = we.ProcessedWorm(0,ep_len=worm_ep_len,act_spacing=1) \n",
    "        # act_spacing here is only for eval episodes\n",
    "    ant = tab.Q_Alpha_Agent_Agent()\n",
    "    runner = mba.WormRunner(ant,worm)\n",
    "    \n",
    "    \n",
    "    for ce in range(collection_eps):\n",
    "        # Collecting random data\n",
    "        #############################\n",
    "        fname = f'collect{ce}.pkl'\n",
    "        if collection_ep_time%worm_ep_len != 0:\n",
    "            raise ValueError('Collection_ep_time is not a multiple of worm_ep_len')\n",
    "        mba.get_init_traj(fname, worm, int(collection_ep_time/worm_ep_len), rand_probs=[1-frac_on,frac_on])\n",
    "        dh.add_dict_to_df([fname],reward_ahead=10,timestep_gap=1,prev_act_window=3,jump_limit=100)\n",
    "\n",
    "        # Find RDiff matrix and collect eval episodes\n",
    "        #############################\n",
    "        for i,ev in enumerate(eval_on_list):\n",
    "            mset = ModelSet(1,frac=1,lp_frac=ev)\n",
    "            mset.make_models(dh,{'lambda':.1,'iters':10})\n",
    "            # Save model\n",
    "            mname = f'mod{ce}_{i}.pkl'\n",
    "            with open(mname,'wb') as f:\n",
    "                pickle.dump(mset.models[0])\n",
    "            \n",
    "            rdiff = np.sign(mset.models[0]['reward_on']-mset.models[0]['reward_off'])\n",
    "            runner.ant.Qtab[:,0] = np.zeros(144)\n",
    "            runner.ant.Qtab[:,1] = rdiff.flatten()\n",
    "            cam,task = init_instruments()\n",
    "            ename = f'eval{ce}_{i}.pkl'\n",
    "            runner.eval_ep(cam,task,ename,steps=eval_ep_time)\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r919",
   "language": "python",
   "name": "r919"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
